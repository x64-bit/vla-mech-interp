# Mechanistic Interpretability for Steering Vision-Language-Action Models in Simulation - pi0.5 guide

This README captures the two concept-discovery workflows covered in this repo:

1. **Activation clustering** – hook the π0/π0.5 FFNs, log the most active neurons, and group their value vectors into semantic clusters.
2. **Latent steering** – capture residual streams for contrastive concept pairs, run them through a sparse autoencoder (SAE), and derive steering vectors.

Both pipelines are designed to be completely reproducible with the scripts already checked into `src/openpi`.

For other guides, like OpenVLA usage and more specific notes on collecting latents, see `docs/`.

## Disclaimer
This README was generated by an LLM based on rough internal notes recorded during rapid prototyping. We believe it accurately reflects our current workflow, but there may be gaps or inaccuracies. Feedback, corrections, and improvements are welcome.

---

## Common Prerequisites

- **Environment:** `uv sync` inside `src/openpi`, then run commands via `uv run …` or inside the project venv.
- **Checkpoints:** Place the converted PyTorch checkpoint for your policy under `data/openpi-assets/checkpoints/<name>/model.safetensors` (or point CLI flags at your custom path).
- **Train configs:** We reused the configs in `src/openpi/src/openpi/training/config.py` (`pi0_libero_*`, `pi05_libero_*`), but any `TrainConfig` works as long as you can load the matching checkpoint.

You will also have to clone the [LIBERO repository](https://github.com/Lifelong-Robot-Learning/LIBERO) into `src/` yourself.

---

## 1. Activation Clustering Pipeline

### 1.1 Dump observation batches

`scripts/extract_ffn_vectors.py` consumes `.npz` files where each file stores a dict compatible with `policy.infer`. We typically dump them straight from a Libero dataloader:

```bash
uv run python - <<'PY'
import pathlib, numpy as np
from openpi.training import config as train_config, data_loader

OUTPUT = pathlib.Path("data/activation_records/pi05_libero_open_close")
OUTPUT.mkdir(parents=True, exist_ok=True)

cfg = train_config.get_config("pi05_libero_open_close")
loader = data_loader.create_data_loader(
    cfg, framework="pytorch", shuffle=False, num_batches=256, skip_norm_stats=False
)

for idx, (obs, _) in enumerate(loader):
    path = OUTPUT / f"batch_{idx:04d}.npz"
    np.savez_compressed(path, obs.to_dict())
    if idx == 255:
        break
PY
```

*Tips*
- Pass `--lerobot-home` or set `HF_LEROBOT_HOME` if your LeRobot datasets live outside the repo.
- Use fewer batches while iterating; the probe only needs ~100–200 batches to surface strong neurons.

### 1.2 Extract FFN projections

```bash
uv run python src/openpi/scripts/extract_ffn_vectors.py \
  --config-name pi05_libero_open_close \
  --checkpoint-dir data/openpi-assets/checkpoints/pi05_libero_open_close \
  --observations-glob "data/activation_records/pi05_libero_open_close/*.npz" \
  --output-path data/activation_records/pi05_open_close_ffn.json \
  --top-k-per-layer 32 \
  --max-batches 200 \
  --store-value-vectors \
  --min-activation 0.6 \
  --device cuda
```

This hooks every Gemma MLP down-projection, tracks the max absolute activation per neuron, and dumps JSON entries like:

```json
{
  "layer_index": 12,
  "neuron_index": 745,
  "max_activation": 1.98,
  "top_tokens": [{"token": "▁open", "logit": 7.3}, …],
  "value_vector": [...]
}
```

We checked our run into `test_outputs/test_ffn_projections.json` for quick smoke-tests.

### 1.3 Cluster neurons

```bash
uv run python src/openpi/scripts/cluster_ffn_vectors.py \
  --projections-path data/activation_records/pi05_open_close_ffn.json \
  --output-path data/activation_records/pi05_open_close_clusters.json \
  --num-clusters 40 \
  --normalize \
  --random-seed 42 \
  --keywords open close drawer \
  --use-semantic-space \
  --model-checkpoint data/openpi-assets/checkpoints/pi05_libero_open_close/model.safetensors \
  --model-config data/openpi-assets/checkpoints/pi05_libero_open_close/config.json \
  --top-k-tokens 20 \
  --device cuda
```

Key options:

- `--use-semantic-space` projects each value vector through the LM head and averages top token embeddings, which made the drawer clusters separate more cleanly than raw weight space.
- `--keywords …` prints a ranked list so you can immediately spot clusters whose decoded tokens mention “open/close”, “stove”, etc.
- Restrict to specific layers with `--layers 12 13` if you only care about the paligemma block we probed.

### 1.4 Post-processing

Outputs land under:

- `data/activation_records/*.json` – raw projections and clustered membership lists.
- `test_outputs/cluster_tsne_visualization.png` – optional t-SNE plot if you run `python src/openpi/src/openpi/cluster_test.py`.

You can feed `pi05_open_close_clusters.json` into whatever downstream tooling you like (e.g., steering configs or notebooks).

---

## 2. Latent Concept Steering Pipeline

This section unifies everything from `docs/latent_steering_pipeline.md` plus the new custom suite catalog in `src/openpi/docs/custom_suites.md`.

### 2.1 Convert & filter Libero data

Start from the Libero `.hdf5` demos and convert them to LeRobot format for each binary concept pair (open/close, on/off, front/back):

```bash
uv run src/openpi/examples/libero/convert_libero90_data_to_lerobot.py \
  --args.input-dir data/libero_filtered/open_close \
  --args.repo-id libero_open_close_lerobot \
  --args.output-dir data/libero_lerobot/libero_open_close_lerobot
```

We keep the filtered folders (`data/libero_filtered/*`) so the exact demo subset is documented.

### 2.2 Build contrast manifests

```bash
python data/utils/build_contrasting_pairs.py \
  --backend lerobot \
  --lerobot-root data/libero_lerobot/libero_open_close_lerobot \
  --task-a-text "open the top drawer of the cabinet" \
  --task-b-text "close the top drawer of the cabinet" \
  --output data/manifests/open_vs_close.json
```

Repeat for:

- `on_vs_off.json`
- `open_vs_close_front_back.json`
- `back_vs_front.json`

Each manifest contains 100 segments (50 per concept) plus the axis stats we summarized via `python - <<'PY' …` (see `custom_suites.md`).

### 2.3 Map segments to dataset indices

```bash
python src/openpi/scripts/map_concept_samples.py \
  --manifest-path data/manifests/open_vs_close.json \
  --output-path data/manifests/open_vs_close_concept_map.json \
  --lerobot-root data/libero_lerobot/libero_open_close_lerobot \
  --repo_id libero_open_close_lerobot
```

The concept maps let us line up manifest segments with precise episode indices during slicing.

### 2.4 Capture residuals

```bash
python -m openpi.analysis.save_residuals \
  --args.checkpoint-path data/openpi-assets/checkpoints/pi05_libero_open_close/model.safetensors \
  --args.config-name pi05_libero_open_close \
  --args.output-dir data/residuals/open_close_run \
  --args.layers 12 \
  --args.batch-size 128 \
  --args.capture-instructions \
  --args.samples-per-file 4096 \
  --args.max-samples-per-layer 50000 \
  --args.device cuda
```

Artifacts:

- Decision tokens → `data/residuals/open_close_run/decision/layer12_chunk*.pt`
- Instruction tokens → `data/residuals/open_close_run/instructions/layer12_chunk*.pt`
- Metadata → `data/residuals/open_close_run/metadata.json`

We repeated this for `open_close_front_back_run` to match the harder workspace.

### 2.5 Slice by concept

```bash
python src/openpi/scripts/slice_residuals_by_concept.py \
  --run-dir data/residuals/open_close_run \
  --concept-map-path data/manifests/open_vs_close_concept_map.json \
  --output-dir data/residuals/open_vs_close_slices \
  --token-type instructions \
  --layers 12 \
  --device cpu
```

This writes tensors like `data/residuals/open_vs_close_slices/layer12/open the top drawer of the cabinet.pt`, one per concept.

### 2.6 Define pair config

Edit `data/latent_steering/layer12_pairs.json` (already populated with our four suites) to point at the freshly sliced files. Example entry:

```json
{
  "name": "open_vs_close",
  "positive_label": "open the top drawer of the cabinet",
  "positive_path": "data/residuals/open_vs_close_slices/layer12/open the top drawer of the cabinet.pt",
  "negative_label": "close the top drawer of the cabinet",
  "negative_path": "data/residuals/open_vs_close_slices/layer12/close the top drawer of the cabinet.pt"
}
```

### 2.7 Compute latent directions

```bash
python src/openpi/scripts/compute_latent_steering.py \
  --args.pairs-config data/latent_steering/layer12_pairs.json \
  --args.output-root data/latent_steering \
  --args.layer 12 \
  --args.device cuda \
  --args.batch-size 512 \
  --args.sae-release gemma-2b-it-res-jb \
  --args.finetuned-sae-path sae_finetuned/sae_pi0_layer12_finetuned.pt
```

Outputs per pair under `data/latent_steering/layer12/<pair>/`:

- `positive_latent_mean.pt` & `negative_latent_mean.pt`
- `latent_direction.pt` (difference vector)
- `residual_direction.pt` (decoded direction)

Cross-pair cosine stats → `data/latent_steering/layer12/pairwise_stats.pt`. Plots summarizing magnitudes & cosines live under `data/latent_steering_finetuned/layer12/*.png`.

### 2.8 Reference catalog

See `src/openpi/docs/custom_suites.md` for a table that ties each suite to its repo, manifests, sample counts, and residual directories. That document stays in sync with the steps above so future runs can reuse our assets.

---

## 3. Suggested Validation Steps

- **Activation pipeline:** run `python src/openpi/src/openpi/cluster_test.py` to ensure the projections file is well-formed and to generate quick t-SNE visualizations.
- **Latent pipeline:** inspect `data/latent_steering/layer12_pairwise_summary.png` and the cosine comparison PNGs in `data/latent_steering_finetuned/layer12/` to confirm directions align with expectations (open vs close should anticorrelate with on vs off, etc.).
- **Bookkeeping:** keep `journal/*.md` updated with the commands you ran (see `journal/2025-12-06-latent-steering.md` for our latest log) so each experiment has a paper trail.
